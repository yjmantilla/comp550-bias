import json
import os
import traceback
import time
from pprint import pprint
import numpy as np
import pandas as pd
import string
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from openai import OpenAI # this is for the synthetic data
import yaml
import glob
import matplotlib.pyplot as plt
from utils import get_closest_identity
def load_json(f):
    with open(f,'r') as f:
        return json.load(f)
    
import yaml
def read_yaml(f):
    with open(f, 'r') as stream:
        try:
            return yaml.safe_load(stream)
        except yaml.YAMLError as exc:
            print(exc)

FULL_CFG=read_yaml('cfg.yml')


data_path = 'data/'
results=glob.glob(data_path+'*_samples.json')
# Load the data
results = [load_json(f) for f in results]
results = [pd.DataFrame.from_dict(r,orient='index') for r in results]
df = pd.concat(results)

cfgs = glob.glob(data_path+'*_cfg.json')
cfgs = [load_json(f) for f in cfgs]
df_cfg = pd.DataFrame(cfgs)

# drop phi for now
#df = df[~df['model'].str.contains('phi')]


def sanitize_model_name(x):
    #return x.lower().replace('lmstudio-community/','').replace('instruct-gguf','').replace('instruct-','').replace('lmstudio-','').replace('lmstudio-community/')
    return x.lower().replace('lmstudio-community/','').replace('instruct-gguf','').replace('instruct-','').replace('lmstudio-','').replace('lm-kit/','')

df['model'] = df['model'].apply(sanitize_model_name)

# add configuration to dataframe
df_cfg['model'] = df_cfg['model'].apply(sanitize_model_name)

dict_cfgs=[]
for _,row in df.iterrows():
    # look for its cfg
    matching_criteria = ['task','language','model','bias','domain','baseline']

    df_temp = df_cfg.copy()
    for c in matching_criteria:
        df_temp = df_temp[df_temp[c]==row[c]]
    assert df_temp.shape[0]==1
    df_temp = df_temp.iloc[0].to_dict()
    dict_cfgs.append(df_temp)
df['dict_cfg']=dict_cfgs


row = df.iloc[0]
row['dict_cfg']['identities']
row['identities']

def get_assignment(row, general_cfg):
    identity_map = {' '.join([v['label']]+v['variants'][row['language']]):v['group'] for k,v in row['dict_cfg']['identities'].items()}
    word_map={v[row['language']]:v['group'] for k,v in row['dict_cfg']['words'].items()}
    all_identities = [v['group'] for k,v in row['dict_cfg']['identities'].items()]
    assert len(identity_map) == len(all_identities)
    assignment = get_closest_identity(row['answer'],row['words'],row['identities'])

    # Make multilingual word map
    general_cfg['tasks'][row['task']]['words']
    multi_word_map = {}

    for k,v in general_cfg['tasks'][row['task']]['words'].items():

        full_word =[]
        for _,v2 in v.items():
            full_word.append(v2)
        full_word = ' '.join(full_word)
        # assume group is at the start before the words in each languages, this will help to compute the scores
        multi_word_map[v[row['language']]]=full_word

    if row['baseline']=='0nope':
        group_map ={}
        for k,v in row['dict_cfg']['identities'].items():
            for id in row['identities']:
                if id in v['variants'][row['language']]:
                    group_def = v['group']
                    group_map[id]=group_def
    else:
        names=[]
        for k,v in row['dict_cfg']['identities'].items():
            names.append(v['variants'][row['language']])
        # if baseline, names should be the same
        name_def = names[0]
        for n in names:
            assert n==name_def
        # if 2 ids, there are two ways to make the baseline (either one or the other poses as the "other" identity, right we are doing this from the the order of the identities)
        group_map={k:v for k,v in zip(row['identities'],all_identities)}
        

    assignment_grouped={w:group_map[id] for w,id in assignment.items()}
    multi_lingual_assignment_grouped = {multi_word_map[w]:group_map[id] for w,id in assignment.items()}
    return multi_lingual_assignment_grouped

df['assignment']=df.apply(get_assignment,axis=1,args=(FULL_CFG,))
df['description']=df['task'].apply(lambda x: FULL_CFG['tasks'][x]['task_label'])
# note in sexual orientation i forgot to assign the words with s and g, will have to correct that hardcoding

def correct_assignment(row):
    correct_map = {'f':'s','m':'g'}
    new_assignment = {}
    enter = False
    for w,a in row['assignment'].items():
        if a in ['s','g'] and w[0] in ['f','m']:
            if not enter:
                print('correcting')
                print(row['assignment'])
            enter = True
            new_w = w.split(' ')
            new_w[0] = correct_map[new_w[0]]
            new_assignment[' '.join(new_w)]=a
        else:
            new_assignment[w]=a
    if enter:
        print(new_assignment)
    return new_assignment

df['assignment']=df.apply(correct_assignment,axis=1) # seems that no longer needed as the cfg has been corrected

## Scores per answer
def score(row):
    points = []
    for w,a in row['assignment'].items():
        if w[0]==a:
            points.append(1)
        else:
            points.append(-1)
    length = len(points)
    return (points, sum(points),length)

df['score']=df.apply(score,axis=1)
df['score_points'] = df['score'].apply(lambda x: x[1])
df['score_length'] = df['score'].apply(lambda x: x[2])
df['score_normalized'] = df['score'].apply(lambda x: x[1]/x[2])

## Scores per task,language,model
order_dims = ['task','language','model']
df_per_tasklangmodel = list(df.groupby(order_dims))
scores_per_tasklangmodel = []
for i, df_task in df_per_tasklangmodel:
    this_data ={}
    print(i)
    print(df_task.shape)
    example_row = df_task.iloc[0]

    w_counts = {w:[] for w in example_row['assignment'].keys()}
    for w,a in example_row['assignment'].items():
        for r,row in df_task.iterrows():
            w_counts[w]+=[row['assignment'][w]]

    w_counts = {w: {x: y for x, y in zip(*np.unique(w_counts[w], return_counts=True))} for w in w_counts.keys()}
    # in french horrible and awful were mapped to horrible, so we have to remove one of them
    # and to maintain the same number of words, we remove one with a positive valence
    # remove horrible horrible horrible (and marvelous sublime merveille)
    w_counts = {k:v for k,v in w_counts.items() if 'horrible horrible' not in k and  'marvelous sublime' not in k}
    w_points = {w: [-1*v*(k!=w[0])+1*v*(k==w[0]) for k,v in c.items()] for w,c in w_counts.items()}
    w_sum_points = {w: sum(v) for w,v in w_points.items()}
    w_sum_points_normalized = {w: sum(v)/sum([abs(x) for x in v]) for w,v in w_points.items()}
    w_follow = {w: c.get(w[0],0) for w,c in w_counts.items()}
    w_unfollow = {w: sum([-1*v*(k!=w[0])+0*v*(k==w[0]) for k,v in c.items()]) for w,c in w_counts.items()}
    w_follow_ratio = {w: -0.5 + c.get(w[0],0)/sum(c.values()) for w,c in w_counts.items()} # offset for it to be around 0 for unbiased

    for order_dim in order_dims:
        this_data[order_dim] = example_row[order_dim]
    this_data['bias'] = example_row['bias']
    this_data['domain'] = example_row['domain']
    this_data['baseline'] = example_row['baseline']
    this_data['description'] = example_row['description']
    this_data['n_samples'] = df_task.shape[0]
    this_data['n_words'] = len(w_counts)
    this_data['w_counts'] = w_counts
    this_data['w_points'] = w_points
    this_data['w_sum_points'] = w_sum_points
    this_data['w_sum_points_normalized'] = w_sum_points_normalized
    this_data['w_points_macro']=[sum(v) for k,v in w_points.items()]
    this_data['w_sum_points_macro'] = sum(this_data['w_points_macro'])
    this_data['w_sum_points_normalized_macro'] = sum([sum(v) for k,v in w_points.items()])/sum([sum([abs(x) for x in v]) for k,v in w_points.items()])
    this_data['w_follow'] = w_follow
    this_data['w_unfollow'] = w_unfollow
    this_data['w_follow_ratio'] = w_follow_ratio
    this_data['w_follow_macro'] = sum(w_follow.values())
    this_data['w_unfollow_macro'] = sum(w_unfollow.values())
    this_data['w_follow_ratio_macro'] = (sum(w_follow.values())/sum([sum(c.values()) for c in w_counts.values()]) -0.5) # offset for it to be around 0 for unbiased
    scores_per_tasklangmodel.append(this_data)

df_scores = pd.DataFrame(scores_per_tasklangmodel)

# drop power french as mistake in gendered words

df_scores = df_scores[~((df_scores['domain']=='power') & (df_scores['language']=='french'))]

# no french for phi model
df_scores = df_scores[~(df_scores['model'].str.contains('phi') & (df_scores['language']=='french'))]

# exlude sexuality for now

df_scores = df_scores[~(df_scores['bias'].str.contains('sexuality'))]

order_dims = ['bias','domain']
df_scores_per_biasdomain = list(df_scores.groupby(order_dims))


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

for i, df_task in df_scores_per_biasdomain:
    print(i)
    bias=df_task.iloc[0]['bias']
    domain=df_task.iloc[0]['domain']
    description=df_task.iloc[0]['description']
    group2label_map = {v['group']:v['label'] for k,v in FULL_CFG['tasks'][df_task.iloc[0]['task']]['identities'].items()}
    print(df_task.shape)
    print(df_task.iloc[0])

    # Sort data for hierarchical structure
    baseline_mapping ={}
    first=True
    for k in df_task['baseline'].copy().unique().tolist():
        
        if k=='0nope':
            baseline_mapping[k]='0'
        else:
            if first:
                baseline_mapping[k]='-1'
                first=False
            else:
                baseline_mapping[k]='1'


    # Sort and order the data
    df_task['baseline_order'] = df_task['baseline'].apply(lambda x: baseline_mapping[x])
    df_task = df_task.sort_values(['model', 'language', 'baseline_order']).reset_index(drop=True)

    # Create artificial empty rows for spacing
    rows = []
    previous_model = None
    previous_language = None

    for _, row in df_task.iterrows():
        # Add 2 empty rows when the model changes
        if previous_model is not None and row['model'] != previous_model:
            rows.append({**row, 'w_follow_ratio_macro': None, 'x_label': '', 'baseline': None})
            rows.append({**row, 'w_follow_ratio_macro': None, 'x_label': '', 'baseline': None})
        # Add 1 empty row when the language changes
        elif previous_language is not None and row['language'] != previous_language:
            rows.append({**row, 'w_follow_ratio_macro': None, 'x_label': f'\n\n {previous_model}', 'baseline': None})
        
        #rows.append(row.to_dict())
        
        # Modify x_label conditionally: display the language if baseline_order == 0
        x_label = '\n'+row['language'] if row['baseline_order'] == '0' else ''
        rows.append({**row, 'x_label': x_label})

        previous_model = row['model']
        previous_language = row['language']

    # Convert rows back to DataFrame and reset x positions
    df_spaced = pd.DataFrame(rows)
    df_spaced['x_pos'] = range(len(df_spaced))

    # Plot with Seaborn
    sns.set_theme(style="whitegrid")
    plt.figure(figsize=(14, 8))

    # Barplot with artificial spaces
    sns.barplot(
        data=df_spaced,
        x="x_pos", 
        y="w_follow_ratio_macro",
        hue="description",
        palette="tab10",
        dodge=False,
    )

    # Replace x-axis labels (exclude artificial empty labels)
    plt.xticks(
        ticks=df_spaced['x_pos'], 
        labels=df_spaced['x_label'].fillna(''), 
        rotation=45, ha="right"
    )
    
    # set max and min values
    plt.ylim(-0.5,0.5)

    # Customize layout
    plt.suptitle(f"Bias for {description}")
    plt.title("Bias in [-0.5,0.5]. Positive reinforces a stereotype, negative the opposite. Unbiased is 0.")
    plt.xlabel("Language | Model")
    plt.ylabel("Bias")

    plt.tight_layout()

    # Save or show plot
    os.makedirs('output', exist_ok=True)
    plt.savefig(f"output/hierarchical_barplot_{bias}_{domain}.png")
    plt.close('all')

    ## Score per word heatmap

    # Flatten word data from w_follow_ratio into long-form DataFrame
    word_rows = []
    for _, row in df_task.iterrows():
        for word, ratio in row['w_follow_ratio'].items():
            word_rows.append({
                'word': word,
                'model': row['model'],
                'language': row['language'],
                'baseline_order': baseline_mapping[row['baseline']],
                'baseline': row['baseline'],
                'description': row['description'],
                'w_follow_ratio': ratio
            })
    df_words = pd.DataFrame(word_rows)


    # Sort data for hierarchy
    df_words = df_words.sort_values(['model', 'word','language', 'baseline_order']).reset_index(drop=True)

    # Add artificial columns for spacing
    columns = []
    previous_model = None
    previous_language = None

    non_unique_spaces = 0
    for i, row in df_words.iterrows():
        col_label = f"{row['model']}\n{row['language']}\n{baseline_mapping[row['baseline']]}\n{row['description']}"
        columns.append({'word': row['word'], 'col_label': col_label, 'w_follow_ratio': row['w_follow_ratio']}) # offset for it to be around 0 for unbiased
        previous_model = row['model']
        previous_language = row['language']

    # Convert to DataFrame and pivot to wide format
    df_heatmap = pd.DataFrame(columns)
    heatmap_data = df_heatmap.pivot(index="word", columns="col_label", values="w_follow_ratio")


    # Create a new index with empty rows
    new_index = []
    previous_language = None
    previous_model = None
    top_ticks = []
    ticks =[]
    for col in heatmap_data.columns:
        print(col)
        language  = col.split('\n')[1]
        model = col.split('\n')[0]
        # Add 2 empty rows when the model changes
        if previous_model is not None and col.split('\n')[0] != previous_model:
            new_index.append(f' ')
            new_index.append(f' ')
            heatmap_data[' ']=np.zeros_like(heatmap_data.iloc[:,0])
            top_ticks.append(' ')
            top_ticks.append(' ')
            ticks.append(' ')
            ticks.append(' ')
        # Add 1 empty row when the language changes
        elif previous_language is not None and col.split('\n')[1] != previous_language:
            new_index.append(' ')
            top_ticks.append(model)
            heatmap_data[' ']=np.zeros_like(heatmap_data.iloc[:,0])
            ticks.append(' ')
        border = col.split('\n')[2]
        desc = col.split('\n')[3]
        if border=='0':
            key=desc#+f'\n{language}'
            new_index.append(col)
            #heatmap_data[key]=heatmap_data[col]
            top_ticks.append(language)
            ticks.append(key)
        else:
            key=desc
            new_index.append(col)
            #heatmap_data[key]=heatmap_data[col]
            top_ticks.append(' ')
            ticks.append(key)

        # Update previous values
        previous_language = col.split('\n')[1]
        previous_model = col.split('\n')[0]


    heatmap_data_with_spacing = heatmap_data[new_index]

    heatmap_data_with_spacing.index = heatmap_data_with_spacing.index = [
        '('+group2label_map[x[0]] +')'+ x[1:] for x in heatmap_data_with_spacing.index
    ]



    max_abs_value = max(abs(heatmap_data_with_spacing.min().min()), abs(heatmap_data_with_spacing.max().max()))

    # Plot the heatmap
    sns.set_theme(style="whitegrid")
    plt.figure(figsize=(16, 10))
    ax = sns.heatmap(
        heatmap_data_with_spacing, 
        cmap="seismic", 
        annot=False, 
        linewidths=0.5, 
        linecolor='gray', 
        vmin=-0.5,#-max_abs_value,  # Minimum value for color scale
        vmax=0.5,#max_abs_value,   # Maximum value for color scale
        cbar_kws={'label': 'bias'},
    )


    # Add secondary x-axis on top with different tick labels
    ax_top = ax.secondary_xaxis('top')  # Create a top axis
    top_labels = top_ticks
    ax_top.set_xticks(ax.get_xticks())  # Align top ticks with the bottom
    ax_top.set_xticklabels(top_labels, rotation=45, ha="left", fontsize=10)

    # set xticks labels
    ax.set_xticklabels(ticks, rotation=45, ha="right")

    # Move title upward and adjust layout
    #plt.title("Word-Level Heatmap of Bias (w_follow_ratio)", pad=30)
    plt.suptitle(f"Word-Level Heatmap of Bias for {description}")
    plt.title("Bias in [-0.5,0.5]. Positive reinforces a stereotype, negative the opposite. Unbiased is 0.\n In parentheses the stereotype assigned to the word.")
    plt.xlabel("")#Baseline | Language | Model", labelpad=20)
    plt.ylabel("Words")

    # Save or show plot
    plt.tight_layout()
    #plt.show()
    plt.savefig(f"output/word_bias_heatmap_with_spacing_{bias}_{domain}.png")

    plt.close('all')

    ##  Transposed version

    heatmap_data_transposed = heatmap_data_with_spacing.T


    # Determine the range of data for the heatmap
    max_abs_value = max(abs(heatmap_data_transposed.min().min()), abs(heatmap_data_transposed.max().max()))

    # Plot the heatmap
    sns.set_theme(style="whitegrid")
    plt.figure(figsize=(10, 16))
    ax = sns.heatmap(
        heatmap_data_transposed, 
        cmap="seismic", 
        annot=False, 
        linewidths=0.5, 
        linecolor='gray', 
        vmin=-0.5,#-max_abs_value,  # Minimum value for color scale
        vmax=0.5,#max_abs_value,   # Maximum value for color scale
        cbar_kws=dict(use_gridspec=True,location="bottom",label='bias',orientation="horizontal"),
    )

    # Bottom ticks (default behavior)
    ax.set_xticklabels(heatmap_data_transposed.columns, rotation=45, ha="right")

    # Add secondary x-axis on top with different tick labels
    ax_top = ax.secondary_yaxis('right')  # Create a top axis
    top_labels = top_ticks
    ax_top.set_yticks(ax.get_yticks())  # Align top ticks with the bottom
    ax_top.set_yticklabels(top_labels, rotation=45, fontsize=10)
    ax.set_yticklabels(ticks, rotation=0, ha="right")

    # Move title upward and adjust layout
    #plt.title("Word-Level Heatmap of Bias (w_follow_ratio)", pad=30)
    plt.suptitle(f"Word-Level Heatmap of Bias for {description}")
    plt.title("Bias in [-0.5,0.5]. Positive reinforces a stereotype, negative the opposite. Unbiased is 0.\n In parentheses the stereotype assigned to the word.")
    plt.ylabel("")#Baseline | Language | Model", labelpad=20)
    plt.xlabel("Words")

    # Save or show plot
    plt.tight_layout()
    #plt.show()

    # not needed for now
    #plt.savefig(f"output/word_bias_heatmap_with_spacing_{bias}_{domain}_transposed.png")

    plt.close('all')

## box plots across each dimension


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os

df_task = df_scores.copy()

def basefun(x):
    if x == '0nope':
        return 'Woman vs Man'
    if x == 'm':
        return 'Man vs Man'
    if x == 'f':
        return 'Woman vs Woman'
df_task['description']=df_task['baseline'].apply(lambda x: basefun(x))
group2label_map = {v['group']: v['label'] for k, v in FULL_CFG['tasks'][df_task.iloc[0]['task']]['identities'].items()}
print(df_task.shape)
print(df_task.iloc[0])

# Sort data for hierarchical structure
baseline_mapping = {}
first = True
for k in df_task['baseline'].copy().unique().tolist():
    if k == '0nope':
        baseline_mapping[k] = '0'
    else:
        if first:
            baseline_mapping[k] = '-1'
            first = False
        else:
            baseline_mapping[k] = '1'

# Sort and order the data
df_task['baseline_order'] = df_task['baseline'].apply(lambda x: baseline_mapping[x])
df_task = df_task.sort_values(['model', 'language', 'baseline_order']).reset_index(drop=True)

# Plot for each varying dimension
for varying_dimension in ['model', 'language','domain']:
    sns.set_theme(style="whitegrid")
    plt.figure(figsize=(14, 8))

    # Boxplot grouped by the varying dimension
    sns.boxplot(
        data=df_task,
        x=varying_dimension,  # Varying dimension on the x-axis
        y="w_follow_ratio_macro",  # Distribution values
        hue="description",  # Use hue to distinguish baselines
        palette="tab10"
    )

    # Customize layout
    plt.suptitle(f"Distribution of Bias by {varying_dimension.capitalize()}")
    plt.title("Bias in [-0.5,0.5]. Positive reinforces a stereotype, negative the opposite. Unbiased is 0.")
    plt.xlabel(varying_dimension.capitalize())
    plt.ylabel("Bias")
    plt.ylim(-0.5, 0.5)
    plt.legend(title="", loc="upper right")

    # Save or show plot
    os.makedirs('output', exist_ok=True)
    plt.savefig(f"output/boxplot_bias_{varying_dimension}.png")
    plt.close('all')

