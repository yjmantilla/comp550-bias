@misc{wang_measuring_2024,
	title = {Measuring machine learning harms from stereotypes: requires understanding who is being harmed by which errors in what ways},
	url = {http://arxiv.org/abs/2402.04420},
	doi = {10.48550/arXiv.2402.04420},
	shorttitle = {Measuring machine learning harms from stereotypes},
	abstract = {As machine learning applications proliferate, we need an understanding of their potential for harm. However, current fairness metrics are rarely grounded in human psychological experiences of harm. Drawing on the social psychology of stereotypes, we use a case study of gender stereotypes in image search to examine how people react to machine learning errors. First, we use survey studies to show that not all machine learning errors reflect stereotypes nor are equally harmful. Then, in experimental studies we randomly expose participants to stereotype-reinforcing, -violating, and -neutral machine learning errors. We find stereotype-reinforcing errors induce more experientially (i.e., subjectively) harmful experiences, while having minimal changes to cognitive beliefs, attitudes, or behaviors. This experiential harm impacts women more than men. However, certain stereotype-violating errors are more experientially harmful for men, potentially due to perceived threats to masculinity. We conclude that harm cannot be the sole guide in fairness mitigation, and propose a nuanced perspective depending on who is experiencing what harm and why.},
	number = {{arXiv}:2402.04420},
	publisher = {{arXiv}},
	author = {Wang, Angelina and Bai, Xuechunzi and Barocas, Solon and Blodgett, Su Lin},
	urldate = {2024-05-24},
	date = {2024-02-06},
	eprinttype = {arxiv},
	eprint = {2402.04420 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv.org Snapshot:/Users/katerinamcmullen/Zotero/storage/GX3KFSTU/2402.html:text/html},
}

@misc{lucy_one-size-fits-all_2024,
	title = {"One-Size-Fits-All"? Examining Expectations around What Constitute "Fair" or "Good" {NLG} System Behaviors},
	url = {http://arxiv.org/abs/2310.15398},
	doi = {10.48550/arXiv.2310.15398},
	shorttitle = {"One-Size-Fits-All"?},
	abstract = {Fairness-related assumptions about what constitute appropriate {NLG} system behaviors range from invariance, where systems are expected to behave identically for social groups, to adaptation, where behaviors should instead vary across them. To illuminate tensions around invariance and adaptation, we conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in {NLG} system inputs. Through these cases studies, we examine people's expectations of system behaviors, and surface potential caveats of these contrasting yet commonly held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; in contrast, motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for {NLG} systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around what constitute "fair" or "good" {NLG} system behaviors.},
	number = {{arXiv}:2310.15398},
	publisher = {{arXiv}},
	author = {Lucy, Li and Blodgett, Su Lin and Shokouhi, Milad and Wallach, Hanna and Olteanu, Alexandra},
	urldate = {2024-05-24},
	date = {2024-04-03},
	eprinttype = {arxiv},
	eprint = {2310.15398 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Human-Computer Interaction},
	file = {arXiv.org Snapshot:/Users/katerinamcmullen/Zotero/storage/MPNU6ACM/2310.html:text/html},
}

@misc{bai_measuring_2024,
	title = {Measuring Implicit Bias in Explicitly Unbiased Large Language Models},
	url = {http://arxiv.org/abs/2402.04105},
	doi = {10.48550/arXiv.2402.04105},
	abstract = {Large language models ({LLMs}) can pass explicit social bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as {LLMs} become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both challenges by introducing two new measures of bias: {LLM} Implicit Bias, a prompt-based method for revealing implicit bias; and {LLM} Decision Bias, a strategy to detect subtle discrimination in decision-making tasks. Both measures are based on psychological research: {LLM} Implicit Bias adapts the Implicit Association Test, widely used to study the automatic associations between concepts held in human minds; and {LLM} Decision Bias operationalizes psychological results indicating that relative evaluations between two candidates, not absolute evaluations assessing each independently, are more diagnostic of implicit biases. Using these measures, we found pervasive stereotype biases mirroring those in society in 8 value-aligned models across 4 social categories (race, gender, religion, health) in 21 stereotypes (such as race and criminality, race and weapons, gender and science, age and negativity). Our prompt-based {LLM} Implicit Bias measure correlates with existing language model embedding-based bias methods, but better predicts downstream behaviors measured by {LLM} Decision Bias. These new prompt-based measures draw from psychology's long history of research into measuring stereotype biases based on purely observable behavior; they expose nuanced biases in proprietary value-aligned {LLMs} that appear unbiased according to standard benchmarks.},
	number = {{arXiv}:2402.04105},
	publisher = {{arXiv}},
	author = {Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and Griffiths, Thomas L.},
	urldate = {2024-11-08},
	date = {2024-05-23},
	eprinttype = {arxiv},
	eprint = {2402.04105},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society},
	file = {Snapshot:/Users/katerinamcmullen/Zotero/storage/L78XLB2U/2402.html:text/html},
}

@inproceedings{kotek_gender_2023,
	location = {Delft Netherlands},
	title = {Gender bias and stereotypes in Large Language Models},
	isbn = {9798400701139},
	url = {https://dl.acm.org/doi/10.1145/3582269.3615599},
	doi = {10.1145/3582269.3615599},
	eventtitle = {{CI} '23: Collective Intelligence Conference},
	pages = {12--24},
	booktitle = {Proceedings of The {ACM} Collective Intelligence Conference},
	publisher = {{ACM}},
	author = {Kotek, Hadas and Dockum, Rikker and Sun, David},
	urldate = {2024-11-08},
	date = {2023-11-06},
	langid = {english},
}

@misc{blodgett2020languagetechnologypowercritical,
      title={Language (Technology) is Power: A Critical Survey of Bias in NLP}, 
      author={Su Lin Blodgett and Solon Barocas and Hal Daumé III au2 and Hanna Wallach},
      year={2020},
      eprint={2005.14050},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14050}, 
}

@inproceedings{blodgett_language_2020,
	location = {Online},
	title = {Language (Technology) is Power: A Critical Survey of “Bias” in {NLP}},
	url = {https://aclanthology.org/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	shorttitle = {Language (Technology) is Power},
	abstract = {We survey 146 papers analyzing “bias” in {NLP} systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of {NLP}. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in {NLP} systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”—i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by {NLP} systems, while interrogating and reimagining the power relations between technologists and such communities.},
	eventtitle = {{ACL} 2020},
	pages = {5454--5476},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé {III}, Hal and Wallach, Hanna},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2024-11-08},
	date = {2020-07},
}

@misc{glazko_identifying_2024,
	title = {Identifying and Improving Disability Bias in {GPT}-Based Resume Screening},
	url = {http://arxiv.org/abs/2402.01732},
	abstract = {As Generative {AI} rises in adoption, its use has expanded to include domains such as hiring and recruiting. However, without examining the potential of bias, this may negatively impact marginalized populations, including people with disabilities. To address this important concern, we present a resume audit study, in which we ask {ChatGPT} (specifically, {GPT}-4) to rank a resume against the same resume enhanced with an additional leadership award, scholarship, panel presentation, and membership that are disability related. We find that {GPT}-4 exhibits prejudice towards these enhanced {CVs}. Further, we show that this prejudice can be quantifiably reduced by training a custom {GPTs} on principles of {DEI} and disability justice. Our study also includes a unique qualitative analysis of the types of direct and indirect ableism {GPT}-4 uses to justify its biased decisions and suggest directions for additional bias mitigation work. Additionally, since these justifications are presumably drawn from training data containing real-world biased statements made by humans, our analysis suggests additional avenues for understanding and addressing human bias.},
	number = {{arXiv}:2402.01732},
	publisher = {{arXiv}},
	author = {Glazko, Kate and Mohammed, Yusuf and Kosa, Ben and Potluri, Venkatesh and Mankoff, Jennifer},
	urldate = {2024-11-08},
	date = {2024-05-22},
	eprinttype = {arxiv},
	eprint = {2402.01732},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Snapshot:/Users/katerinamcmullen/Zotero/storage/Z3Z3F54C/2402.html:text/html},
}
